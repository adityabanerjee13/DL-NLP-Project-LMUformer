{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEST Statistics:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification Report:**\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.61      0.66      0.63      6000\n",
    "           1       0.76      0.78      0.77      6000\n",
    "           2       0.77      0.84      0.81      6000\n",
    "           3       0.61      0.57      0.59      6000\n",
    "           4       0.84      0.90      0.87      6000\n",
    "           5       0.92      0.91      0.91      6000\n",
    "           6       0.69      0.50      0.58      6000\n",
    "           7       0.76      0.75      0.75      6000\n",
    "           8       0.75      0.81      0.78      6000\n",
    "           9       0.80      0.80      0.80      6000\n",
    "\n",
    "**total dataset size** = 60000\n",
    "\n",
    "**accuracy** = 0.75    \n",
    " \n",
    "**macro avg**  \n",
    "**precision** = 0.75      \n",
    "**recall** = 0.75      \n",
    "**f1-score** = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading datasets...\n",
      "Training with 100000 examples\n",
      "Loading BERT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3125/3125 [18:23<00:00,  2.83it/s, loss=1.3548]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.8835\n",
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [06:41<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.57      0.62      6000\n",
      "           1       0.75      0.79      0.77      6000\n",
      "           2       0.77      0.83      0.80      6000\n",
      "           3       0.66      0.50      0.57      6000\n",
      "           4       0.86      0.89      0.87      6000\n",
      "           5       0.91      0.91      0.91      6000\n",
      "           6       0.65      0.53      0.59      6000\n",
      "           7       0.66      0.80      0.72      6000\n",
      "           8       0.68      0.85      0.76      6000\n",
      "           9       0.83      0.77      0.80      6000\n",
      "\n",
      "    accuracy                           0.75     60000\n",
      "   macro avg       0.74      0.75      0.74     60000\n",
      "weighted avg       0.74      0.75      0.74     60000\n",
      "\n",
      "Saved best model with accuracy: 0.7400\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3125/3125 [18:34<00:00,  2.80it/s, loss=0.8915]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.7221\n",
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [06:38<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.66      0.63      6000\n",
      "           1       0.76      0.78      0.77      6000\n",
      "           2       0.77      0.84      0.81      6000\n",
      "           3       0.61      0.57      0.59      6000\n",
      "           4       0.84      0.90      0.87      6000\n",
      "           5       0.92      0.91      0.91      6000\n",
      "           6       0.69      0.50      0.58      6000\n",
      "           7       0.76      0.75      0.75      6000\n",
      "           8       0.75      0.81      0.78      6000\n",
      "           9       0.80      0.80      0.80      6000\n",
      "\n",
      "    accuracy                           0.75     60000\n",
      "   macro avg       0.75      0.75      0.75     60000\n",
      "weighted avg       0.75      0.75      0.75     60000\n",
      "\n",
      "Saved best model with accuracy: 0.7500\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3125/3125 [18:42<00:00,  2.78it/s, loss=0.9077]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.6485\n",
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [06:12<00:00,  5.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.64      0.64      6000\n",
      "           1       0.74      0.80      0.77      6000\n",
      "           2       0.79      0.82      0.81      6000\n",
      "           3       0.63      0.54      0.59      6000\n",
      "           4       0.85      0.90      0.87      6000\n",
      "           5       0.91      0.92      0.91      6000\n",
      "           6       0.67      0.52      0.58      6000\n",
      "           7       0.75      0.76      0.75      6000\n",
      "           8       0.74      0.82      0.78      6000\n",
      "           9       0.80      0.81      0.80      6000\n",
      "\n",
      "    accuracy                           0.75     60000\n",
      "   macro avg       0.75      0.75      0.75     60000\n",
      "weighted avg       0.75      0.75      0.75     60000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class YahooDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Combine text fields and clean up\n",
    "        self.texts = (dataframe['question_title'].fillna('') + ' [SEP] ' + \n",
    "                     dataframe['question_content'].fillna('') + ' [SEP] ' + \n",
    "                     dataframe['best_answer'].fillna(''))\n",
    "        \n",
    "        # Convert to zero-based indexing\n",
    "        self.labels = dataframe['class'] - 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts.iloc[idx])\n",
    "        label = self.labels.iloc[idx]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc='Training')\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Evaluating'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model.forward(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "    \n",
    "    return classification_report(true_labels, predictions, zero_division=0)\n",
    "\n",
    "def main():\n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Using device: {device}')\n",
    "    \n",
    "    # Load a subset of training data\n",
    "    print('Loading datasets...')\n",
    "    train_df = pd.read_csv('train.csv', \n",
    "                          names=['class', 'question_title', 'question_content', 'best_answer'])\n",
    "    \n",
    "    # Sample 10,000 examples per class for balanced training\n",
    "    samples_per_class = 10000\n",
    "    sampled_train_df = []\n",
    "    \n",
    "    for class_idx in range(1, 11):  # 10 classes\n",
    "        class_data = train_df[train_df['class'] == class_idx]\n",
    "        sampled_class = class_data.sample(n=min(samples_per_class, len(class_data)), \n",
    "                                        random_state=42)\n",
    "        sampled_train_df.append(sampled_class)\n",
    "    \n",
    "    train_df = pd.concat(sampled_train_df, ignore_index=True)\n",
    "    print(f'Training with {len(train_df)} examples')\n",
    "    \n",
    "    # Load test data\n",
    "    test_df = pd.read_csv('test.csv', \n",
    "                         names=['class', 'question_title', 'question_content', 'best_answer'])\n",
    "    \n",
    "    # Load classes\n",
    "    with open('classes.txt', 'r') as f:\n",
    "        class_names = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    # Initialize tokenizer and model\n",
    "    print('Loading BERT model...')\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        'bert-base-uncased',\n",
    "        num_labels=len(class_names),\n",
    "        problem_type=\"single_label_classification\"\n",
    "    ).to(device)\n",
    "    \n",
    "    # Freeze most of the layers\n",
    "    for name, param in model.bert.named_parameters():\n",
    "        if 'encoder.layer' in name:\n",
    "            layer_num = int(name.split('.')[2])\n",
    "            if layer_num < 9:  # Freeze first 9 layers\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = YahooDataset(train_df, tokenizer)\n",
    "    test_dataset = YahooDataset(test_df, tokenizer)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    batch_size = 32  # Increased batch size\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=RandomSampler(train_dataset)\n",
    "    )\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Training settings\n",
    "    epochs = 3\n",
    "    optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    print('Starting training...')\n",
    "    best_accuracy = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f'\\nEpoch {epoch + 1}/{epochs}')\n",
    "        \n",
    "        train_loss = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
    "        print(f'Average training loss: {train_loss:.4f}')\n",
    "        \n",
    "        print('\\nEvaluating...')\n",
    "        report = evaluate(model, test_loader, device)\n",
    "        print('\\nClassification Report:')\n",
    "        print(report)\n",
    "        \n",
    "        # Save model if it improves\n",
    "        accuracy = float(report.split('\\n')[-2].split()[-2])\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), 'best_yahoo_bert_efficient.pt')\n",
    "            print(f'Saved best model with accuracy: {accuracy:.4f}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aditya-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
