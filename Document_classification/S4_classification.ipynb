{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEST Statistics:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification Report:**\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.54      0.63      0.58      6000\n",
    "           1       0.74      0.71      0.72      6000\n",
    "           2       0.77      0.77      0.77      6000\n",
    "           3       0.60      0.44      0.51      6000\n",
    "           4       0.84      0.86      0.85      6000\n",
    "           5       0.88      0.86      0.87      6000\n",
    "           6       0.63      0.48      0.54      6000\n",
    "           7       0.65      0.73      0.69      6000\n",
    "           8       0.68      0.79      0.73      6000\n",
    "           9       0.72      0.79      0.76      6000\n",
    "\n",
    "**total dataset size** = 60000\n",
    "\n",
    "**accuracy** = 0.71    \n",
    " \n",
    "**macro avg**  \n",
    "**precision** = 0.70      \n",
    "**recall** = 0.71      \n",
    "**f1-score** = 0.70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/aditya-venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-12-01 10:01:39.971502: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-01 10:01:39.985741: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733047300.000331 1353697 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733047300.004704 1353697 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-01 10:01:40.019463: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import fft\n",
    "import torch\n",
    "import numpy as np\n",
    "# from spikingjelly.clock_driven.neuron import MultiStepLIFNode\n",
    "# from spikingjelly.clock_driven import functional\n",
    "from scipy.signal import cont2discrete\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def get_model_size(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    param_size = 4  # Assuming float32 (4 bytes per parameter)\n",
    "    total_size = total_params * param_size\n",
    "    return total_size / (1024 ** 2)  # Convert bytes to MB\n",
    "\n",
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n",
    "\n",
    "class DropoutNd(nn.Module):\n",
    "    def __init__(self, p: float = 0.5, tie=True, transposed=True):\n",
    "        \"\"\"\n",
    "        tie: tie dropout mask across sequence lengths (Dropout1d/2d/3d)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if p < 0 or p >= 1:\n",
    "            raise ValueError(\"dropout probability has to be in [0, 1), \" \"but got {}\".format(p))\n",
    "        self.p = p\n",
    "        self.tie = tie\n",
    "        self.transposed = transposed\n",
    "        self.binomial = torch.distributions.binomial.Binomial(probs=1-self.p)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"X: (batch, dim, lengths...).\"\"\"\n",
    "        if self.training:\n",
    "            if not self.transposed: X = rearrange(X, 'b ... d -> b d ...')\n",
    "            # binomial = torch.distributions.binomial.Binomial(probs=1-self.p) # This is incredibly slow because of CPU -> GPU copying\n",
    "            mask_shape = X.shape[:2] + (1,)*(X.ndim-2) if self.tie else X.shape\n",
    "            # mask = self.binomial.sample(mask_shape)\n",
    "            mask = torch.rand(*mask_shape, device=X.device) < 1.-self.p\n",
    "            X = X * mask * (1.0/(1-self.p))\n",
    "            if not self.transposed: X = rearrange(X, 'b d ... -> b ... d')\n",
    "            return X\n",
    "        return X\n",
    "\n",
    "class S4DKernel(nn.Module):\n",
    "    \"\"\"Generate convolution kernel from diagonal SSM parameters.\"\"\"\n",
    "    def __init__(self, d_model, N=64, dt_min=0.001, dt_max=0.1, lr=None):\n",
    "        super().__init__()\n",
    "        # Generate dt\n",
    "        # print(N, dt_max, dt_min)\n",
    "        H = d_model\n",
    "        log_dt = torch.rand(H) * ( math.log(dt_max) - math.log(dt_min) ) + math.log(dt_min)\n",
    "\n",
    "        C = torch.randn(H, N // 2, dtype=torch.cfloat)\n",
    "        self.C = nn.Parameter(torch.view_as_real(C))\n",
    "        self.register(\"log_dt\", log_dt, lr)\n",
    "\n",
    "        log_A_real = torch.log(0.5 * torch.ones(H, N//2))\n",
    "        A_imag = math.pi * repeat(torch.arange(N//2), 'n -> h n', h=H)\n",
    "        self.register(\"log_A_real\", log_A_real, lr)\n",
    "        self.register(\"A_imag\", A_imag, lr)\n",
    "\n",
    "    def forward(self, L):\n",
    "        \"\"\"\n",
    "        returns: (..., c, L) where c is number of channels (default 1)\n",
    "        \"\"\"\n",
    "\n",
    "        # Materialize parameters\n",
    "        dt = torch.exp(self.log_dt) # (H)\n",
    "        C = torch.view_as_complex(self.C) # (H N)\n",
    "        A = -torch.exp(self.log_A_real) + 1j * self.A_imag # (H N)\n",
    "\n",
    "        # Vandermonde multiplication\n",
    "        dtA = A * dt.unsqueeze(-1)  # (H N)\n",
    "        K = dtA.unsqueeze(-1) * torch.arange(L, device=A.device) # (H N L)\n",
    "        C = C * (torch.exp(dtA)-1.) / A\n",
    "        K = 2 * torch.einsum('hn, hnl -> hl', C, torch.exp(K)).real\n",
    "\n",
    "        return K\n",
    "\n",
    "    def register(self, name, tensor, lr=None):\n",
    "        \"\"\"Register a tensor with a configurable learning rate and 0 weight decay\"\"\"\n",
    "\n",
    "        if lr == 0.0:\n",
    "            self.register_buffer(name, tensor)\n",
    "        else:\n",
    "            self.register_parameter(name, nn.Parameter(tensor))\n",
    "\n",
    "            optim = {\"weight_decay\": 0.0}\n",
    "            if lr is not None: optim[\"lr\"] = lr\n",
    "            setattr(getattr(self, name), \"_optim\", optim)\n",
    "\n",
    "class S4D(nn.Module):\n",
    "    def __init__(self, d_model, d_state=64, dropout=0.0, transposed=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.h = d_model\n",
    "        self.n = d_state\n",
    "        self.d_output = self.h\n",
    "        self.transposed = transposed\n",
    "\n",
    "        self.D = nn.Parameter(torch.randn(self.h))\n",
    "\n",
    "        # SSM Kernel\n",
    "        self.kernel = S4DKernel(self.h, N=self.n)\n",
    "\n",
    "        # Pointwise\n",
    "        self.activation = nn.GELU()\n",
    "        # dropout_fn = nn.Dropout2d # NOTE: bugged in PyTorch 1.11\n",
    "        dropout_fn = DropoutNd\n",
    "        self.dropout = dropout_fn(dropout) if dropout > 0.0 else nn.Identity()\n",
    "\n",
    "        # position-wise output transform to mix features\n",
    "        self.output_linear = nn.Sequential(\n",
    "            nn.Conv1d(self.h, 2*self.h, kernel_size=1),\n",
    "            nn.GLU(dim=-2),\n",
    "        )\n",
    "\n",
    "    def forward(self, u, **kwargs): # absorbs return_output and transformer src mask\n",
    "        \"\"\" Input and output shape (B, H, L) \"\"\"\n",
    "        if not self.transposed: u = u.transpose(-1, -2)\n",
    "        L = u.size(-1)\n",
    "\n",
    "        # Compute SSM Kernel\n",
    "        k = self.kernel(L=L) # (H L)\n",
    "\n",
    "        # Convolution\n",
    "        k_f = torch.fft.rfft(k, n=2*L) # (H L)\n",
    "        u_f = torch.fft.rfft(u, n=2*L) # (B H L)\n",
    "        y = torch.fft.irfft(u_f*k_f, n=2*L)[..., :L] # (B H L)\n",
    "\n",
    "        # Compute D term in state space equation - essentially a skip connection\n",
    "        y = y + u * self.D.unsqueeze(-1)\n",
    "\n",
    "        y = self.dropout(self.activation(y))\n",
    "        y = self.output_linear(y)\n",
    "        if not self.transposed: y = y.transpose(-1, -2)\n",
    "        return y, None # Return a dummy state to satisfy this repo's interface, but this can be modified\n",
    "\n",
    "def get_act(act_type = 'spike', **act_params):\n",
    "    '''\n",
    "    act_type :- spike, gelu, relu, identity\n",
    "\n",
    "    output :- class <act_type>\n",
    "    '''\n",
    "    act_type = act_type.lower()\n",
    "    # if act_type == 'spike':\n",
    "    #     return MultiStepLIFNode(**act_params, backend='cupy')\n",
    "    #     # act_params['init_tau'] = act_params.pop('tau')\n",
    "    #     # return MultiStepParametricLIFNode(**act_params, backend=\"cupy\")\n",
    "    if act_type == 'relu':\n",
    "        return nn.ReLU()\n",
    "    elif act_type == 'gelu':\n",
    "        return nn.GELU()\n",
    "    elif act_type == 'identity':\n",
    "        return nn.Identity()\n",
    "\n",
    "class S4(nn.Module):\n",
    "    def __init__(self, dim, T, use_all_h=True):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.use_all_h = use_all_h\n",
    "        self.S4 = S4D(d_model= dim, dropout=0.1, transposed=False)\n",
    "        self.proj_conv = nn.Conv1d(dim, dim, kernel_size=1, stride=1)\n",
    "        self.proj_bn = nn.BatchNorm1d(dim)\n",
    "        self.proj_lif = get_act('relu', tau=2.0, detach_reset=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(-1,-2).contiguous()\n",
    "        h, h_n = self.S4(x) # B, N, C; B, C, 1\n",
    "\n",
    "        x = h.transpose(-1,-2).contiguous()\n",
    "\n",
    "        x = self.proj_conv(x)\n",
    "        x = self.proj_lif(self.proj_bn(x).permute(2,1,0).contiguous()).permute(2,1,0).contiguous()\n",
    "        return x\n",
    "\n",
    "class LinearFFN(nn.Module):\n",
    "    def __init__(self, in_features, pre_norm=False, hidden_features=None, out_features=None, drop=0., act_type='spike'):\n",
    "        super().__init__()\n",
    "        self.pre_norm = pre_norm\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "\n",
    "        self.fc1_linear  = nn.Linear(in_features, hidden_features)\n",
    "        self.fc1_ln = nn.LayerNorm(hidden_features)\n",
    "        self.fc1_lif = get_act(act_type if act_type == 'spike' else 'gelu', tau=2.0, detach_reset=True)\n",
    "\n",
    "        self.fc2_linear = nn.Linear(hidden_features, out_features)\n",
    "        self.fc2_ln = nn.LayerNorm(out_features)\n",
    "        self.fc2_lif = get_act(act_type, tau=2.0, detach_reset=True)\n",
    " \n",
    "        self.c_hidden = hidden_features\n",
    "        self.c_output = out_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,C,N = x.shape\n",
    "        # \n",
    "        x = x.permute(0,2,1) # B, N, C\n",
    "        # x = x.reshape(B*N, C)\n",
    "        if self.pre_norm:\n",
    "            x = self.fc1_ln(x)\n",
    "            x = self.fc1_lif(x)\n",
    "            x = self.fc1_linear(x)\n",
    "            \n",
    "            x = self.fc2_ln(x)\n",
    "            x = self.fc2_lif(x)\n",
    "            x = self.fc2_linear(x)\n",
    "\n",
    "        else:\n",
    "            x = self.fc1_linear(x)\n",
    "            x = self.fc1_ln(x)\n",
    "            x = self.fc1_lif(x)\n",
    "\n",
    "            x = self.fc2_linear(x)\n",
    "            x = self.fc2_ln(x)\n",
    "            x = self.fc2_lif(x)\n",
    "\n",
    "        # x = x.reshape(B, N, self.c_output)\n",
    "        x = x.permute(0,2,1) # B, C, N\n",
    "        return x\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, T, mlp_ratio=4., act_type='spike'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = S4(dim=dim, T=T)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = LinearFFN(in_features=dim, hidden_features=mlp_hidden_dim, act_type=act_type)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(x)\n",
    "        x = x + self.mlp(x)\n",
    "        return x\n",
    "\n",
    "class perm(nn.Module):\n",
    "    def __init__(self, a, b, c) -> None:\n",
    "        super().__init__()\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.permute(self.a,self.b,self.c).contiguous()\n",
    "\n",
    "def get_conv_block(T, dim, act_type, kernel_size=3, padding=1, groups=1):\n",
    "    return [\n",
    "        perm(0,2,1),\n",
    "        nn.Conv1d(dim, dim, kernel_size=kernel_size, stride=1, padding=padding, groups=groups, bias=False),\n",
    "        nn.BatchNorm1d(dim),\n",
    "        perm(1,2,0),\n",
    "        get_act(act_type, tau=2.0, detach_reset=True),\n",
    "        perm(2,1,0)\n",
    "]\n",
    "\n",
    "class Conv1d4EB(nn.Module):\n",
    "    def __init__(self, T=128, vw_dim=256, act_type='spike'):\n",
    "        super().__init__()\n",
    "\n",
    "        kernel_size = 3\n",
    "        padding = 1\n",
    "        groups = 1\n",
    "        self.proj_conv = nn.ModuleList(\n",
    "            [perm(0,2,1)]+\\\n",
    "            get_conv_block(T, vw_dim, act_type)+\\\n",
    "            get_conv_block(T, vw_dim, act_type, kernel_size=kernel_size, padding=padding, groups=groups)+\\\n",
    "            get_conv_block(T, vw_dim, act_type, kernel_size=kernel_size, padding=padding, groups=groups)+\\\n",
    "            get_conv_block(T, vw_dim, act_type, kernel_size=kernel_size, padding=padding, groups=groups)+\\\n",
    "            [perm(0,2,1)]\n",
    "        )\n",
    "        self.rpe_conv = nn.ModuleList(\n",
    "            [perm(0,2,1)]+\\\n",
    "            get_conv_block(T, vw_dim, act_type, kernel_size=kernel_size, padding=padding, groups=groups)+\\\n",
    "            [perm(0,2,1)]\n",
    "        )\n",
    "        self.act_loss = 0.0\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        for ele in self.proj_conv:\n",
    "            x = ele(x)\n",
    "\n",
    "        x_rpe = x.clone()\n",
    "        for ele in self.rpe_conv:\n",
    "            x_rpe = ele(x_rpe)\n",
    "\n",
    "        x = x + x_rpe\n",
    "        \n",
    "        return x \n",
    "\n",
    "from transformers import BertModel , BertTokenizer\n",
    "\n",
    "class S4_sq_Classifier(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, hidden_size, num_classes, act_type='relu', T=784, test_mode='all_seq',with_head_lif=False):\n",
    "        super().__init__()\n",
    "        self.with_head_lif = with_head_lif\n",
    "        self.test_mode = test_mode\n",
    "\n",
    "        bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.embedding = nn.Embedding.from_pretrained(bert_model.embeddings.word_embeddings.weight,freeze=False)\n",
    "\n",
    "        self.in_layer = nn.Linear(768, hidden_size)\n",
    "\n",
    "        self.patch_embed = Conv1d4EB(T=T, vw_dim=hidden_size, act_type=act_type)\n",
    "\n",
    "        self.block = nn.ModuleList([\n",
    "            Block(dim=hidden_size, T=T, act_type=act_type)\n",
    "            for j in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # classification head\n",
    "        if self.with_head_lif:\n",
    "            self.head_bn = nn.BatchNorm1d(hidden_size)\n",
    "            self.head_lif = get_act(act_type, tau=2.0, detach_reset=True)\n",
    "\n",
    "        self.head = nn.Linear(hidden_size, num_classes)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        for blk in self.block:\n",
    "            x = blk(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, labels=None, infer=False):\n",
    "        self.act_loss = 0.0\n",
    "        x = self.embedding(x)\n",
    "        x = self.in_layer(x)\n",
    "        x = x.permute(0, 2, 1).contiguous()\n",
    "        x = self.forward_features(x)    # b, d, t -> b, d, t\n",
    "\n",
    "        if self.with_head_lif:\n",
    "            x = self.head_bn(x)         # b, d, t \n",
    "            x = self.head_lif(x)        # b, d, t\n",
    "\n",
    "        x = x.permute(0, 2, 1).contiguous()\n",
    "        x = torch.mean(x, 1)\n",
    "        out = self.head(x)\n",
    "        if infer:\n",
    "            return out\n",
    "        \n",
    "        return self.loss(out, labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 24186506\n",
      "Trainable parameters: 24186506\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n",
    "\n",
    "# Example usage\n",
    "model = S4_sq_Classifier(num_classes=10, input_size=512, num_layers=2, hidden_size=128, T=256).to(device)\n",
    "total_params, trainable_params = count_parameters(model)\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading datasets...\n",
      "Training with 100000 examples\n",
      "Loading tokenizer...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class YahooDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=256):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Combine text fields and clean up\n",
    "        self.texts = (dataframe['question_title'].fillna('') + ' [SEP] ' + \n",
    "                     dataframe['question_content'].fillna('') + ' [SEP] ' + \n",
    "                     dataframe['best_answer'].fillna(''))\n",
    "        \n",
    "        # Convert to zero-based indexing\n",
    "        self.labels = dataframe['class'] - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts.iloc[idx])\n",
    "        label = self.labels.iloc[idx]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc='Training')\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        # print(input_ids.shape)\n",
    "        # print(labels.shape)\n",
    "        # print(attention_mask.shape)\n",
    "        loss = model(\n",
    "            input_ids,\n",
    "            labels=labels\n",
    "        )\n",
    "        # print(loss)\n",
    "        # assert False,''\n",
    "                \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Evaluating'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids,\n",
    "                infer=True\n",
    "            )\n",
    "            \n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "    \n",
    "    return classification_report(true_labels, predictions, zero_division=0)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Load datasets\n",
    "print('Loading datasets...')\n",
    "train_df = pd.read_csv('train.csv', \n",
    "                        names=['class', 'question_title', 'question_content', 'best_answer'])\n",
    "\n",
    "# Sample 10,000 examples per class for balanced training\n",
    "samples_per_class = 10000\n",
    "sampled_train_df = []\n",
    "\n",
    "for class_idx in range(1, 11):  # 10 classes\n",
    "    class_data = train_df[train_df['class'] == class_idx]\n",
    "    sampled_class = class_data.sample(n=min(samples_per_class, len(class_data)), \n",
    "                                    random_state=42)\n",
    "    sampled_train_df.append(sampled_class)\n",
    "\n",
    "train_df = pd.concat(sampled_train_df, ignore_index=True)\n",
    "print(f'Training with {len(train_df)} examples')\n",
    "\n",
    "# Load test data\n",
    "test_df = pd.read_csv('test.csv', \n",
    "                        names=['class', 'question_title', 'question_content', 'best_answer'])\n",
    "\n",
    "# Load classes\n",
    "with open('classes.txt', 'r') as f:\n",
    "    class_names = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Initialize tokenizer\n",
    "print('Loading tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "model = S4_sq_Classifier(num_classes=10, input_size=512, num_layers=2, hidden_size=128, T=256).to(device)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = YahooDataset(train_df, tokenizer)\n",
    "test_dataset = YahooDataset(test_df, tokenizer)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32  # Can use larger batch size with smaller model\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    sampler=RandomSampler(train_dataset)\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3125/3125 [03:23<00:00, 15.33it/s, loss=1.0156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 1.2083\n",
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [01:08<00:00, 27.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.55      0.57      6000\n",
      "           1       0.78      0.63      0.70      6000\n",
      "           2       0.75      0.79      0.77      6000\n",
      "           3       0.49      0.55      0.52      6000\n",
      "           4       0.82      0.87      0.84      6000\n",
      "           5       0.87      0.86      0.87      6000\n",
      "           6       0.58      0.48      0.53      6000\n",
      "           7       0.70      0.69      0.69      6000\n",
      "           8       0.70      0.77      0.73      6000\n",
      "           9       0.72      0.79      0.75      6000\n",
      "\n",
      "    accuracy                           0.70     60000\n",
      "   macro avg       0.70      0.70      0.70     60000\n",
      "weighted avg       0.70      0.70      0.70     60000\n",
      "\n",
      "Saved best model with accuracy: 0.7000\n",
      "\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3125/3125 [03:21<00:00, 15.54it/s, loss=1.2821]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.8741\n",
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [01:10<00:00, 26.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.63      0.58      6000\n",
      "           1       0.74      0.71      0.72      6000\n",
      "           2       0.77      0.77      0.77      6000\n",
      "           3       0.60      0.44      0.51      6000\n",
      "           4       0.84      0.86      0.85      6000\n",
      "           5       0.88      0.86      0.87      6000\n",
      "           6       0.63      0.48      0.54      6000\n",
      "           7       0.65      0.73      0.69      6000\n",
      "           8       0.68      0.79      0.73      6000\n",
      "           9       0.72      0.79      0.76      6000\n",
      "\n",
      "    accuracy                           0.71     60000\n",
      "   macro avg       0.70      0.71      0.70     60000\n",
      "weighted avg       0.70      0.71      0.70     60000\n",
      "\n",
      "\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3125/3125 [02:57<00:00, 17.65it/s, loss=0.6946]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.7403\n",
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [01:10<00:00, 26.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.57      0.56      6000\n",
      "           1       0.74      0.62      0.67      6000\n",
      "           2       0.73      0.79      0.76      6000\n",
      "           3       0.45      0.58      0.51      6000\n",
      "           4       0.85      0.83      0.84      6000\n",
      "           5       0.87      0.86      0.87      6000\n",
      "           6       0.56      0.48      0.51      6000\n",
      "           7       0.76      0.63      0.69      6000\n",
      "           8       0.75      0.73      0.74      6000\n",
      "           9       0.71      0.79      0.75      6000\n",
      "\n",
      "    accuracy                           0.69     60000\n",
      "   macro avg       0.70      0.69      0.69     60000\n",
      "weighted avg       0.70      0.69      0.69     60000\n",
      "\n",
      "\n",
      "Epoch 4/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3125/3125 [03:14<00:00, 16.05it/s, loss=0.4865]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.6162\n",
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [01:09<00:00, 26.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.55      0.57      6000\n",
      "           1       0.68      0.72      0.70      6000\n",
      "           2       0.72      0.79      0.75      6000\n",
      "           3       0.61      0.39      0.48      6000\n",
      "           4       0.80      0.87      0.83      6000\n",
      "           5       0.88      0.84      0.86      6000\n",
      "           6       0.54      0.48      0.51      6000\n",
      "           7       0.64      0.72      0.68      6000\n",
      "           8       0.68      0.78      0.73      6000\n",
      "           9       0.73      0.75      0.74      6000\n",
      "\n",
      "    accuracy                           0.69     60000\n",
      "   macro avg       0.69      0.69      0.68     60000\n",
      "weighted avg       0.69      0.69      0.68     60000\n",
      "\n",
      "\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3125/3125 [03:01<00:00, 17.22it/s, loss=0.6127]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.4867\n",
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [01:08<00:00, 27.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.57      0.54      6000\n",
      "           1       0.72      0.62      0.66      6000\n",
      "           2       0.77      0.73      0.75      6000\n",
      "           3       0.49      0.48      0.49      6000\n",
      "           4       0.81      0.85      0.83      6000\n",
      "           5       0.87      0.84      0.86      6000\n",
      "           6       0.55      0.44      0.49      6000\n",
      "           7       0.61      0.70      0.65      6000\n",
      "           8       0.69      0.75      0.72      6000\n",
      "           9       0.70      0.75      0.72      6000\n",
      "\n",
      "    accuracy                           0.67     60000\n",
      "   macro avg       0.67      0.67      0.67     60000\n",
      "weighted avg       0.67      0.67      0.67     60000\n",
      "\n",
      "\n",
      "Epoch 6/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3125/3125 [03:00<00:00, 17.34it/s, loss=0.1810]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.3631\n",
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [01:10<00:00, 26.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.55      0.53      6000\n",
      "           1       0.67      0.65      0.66      6000\n",
      "           2       0.70      0.78      0.74      6000\n",
      "           3       0.50      0.44      0.47      6000\n",
      "           4       0.79      0.85      0.82      6000\n",
      "           5       0.82      0.86      0.84      6000\n",
      "           6       0.54      0.43      0.48      6000\n",
      "           7       0.67      0.64      0.65      6000\n",
      "           8       0.70      0.71      0.70      6000\n",
      "           9       0.70      0.73      0.71      6000\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.66      0.66      0.66     60000\n",
      "weighted avg       0.66      0.66      0.66     60000\n",
      "\n",
      "\n",
      "Epoch 7/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|█████████ | 2843/3125 [02:50<00:16, 16.77it/s, loss=0.2998]"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training settings\n",
    "epochs = 30\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)  # Slightly higher learning rate\n",
    "total_steps = len(train_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "print('Starting training...')\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'\\nEpoch {epoch + 1}/{epochs}')\n",
    "    \n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
    "    print(f'Average training loss: {train_loss:.4f}')\n",
    "    \n",
    "    print('\\nEvaluating...')\n",
    "    report = evaluate(model, test_loader, device)\n",
    "    print('\\nClassification Report:')\n",
    "    print(report)\n",
    "    \n",
    "    # Save model if it improves\n",
    "    accuracy = float(report.split('\\n')[-2].split()[-2])\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.state_dict(), 'best_yahoo_S4_2layer.pt')\n",
    "        print(f'Saved best model with accuracy: {accuracy:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aditya-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
