{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEST Statistics:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification Report:**\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.58      0.58      0.58      6000\n",
    "           1       0.66      0.80      0.73      6000\n",
    "           2       0.75      0.78      0.77      6000\n",
    "           3       0.55      0.49      0.52      6000\n",
    "           4       0.82      0.87      0.84      6000\n",
    "           5       0.88      0.83      0.85      6000\n",
    "           6       0.67      0.45      0.54      6000\n",
    "           7       0.69      0.68      0.68      6000\n",
    "           8       0.63      0.83      0.72      6000\n",
    "           9       0.81      0.69      0.74      6000\n",
    "    \n",
    "**total dataset size** = 60000\n",
    "\n",
    "**accuracy** = 0.70    \n",
    " \n",
    "**macro avg**  \n",
    "**precision** = 0.70      \n",
    "**recall** = 0.70      \n",
    "**f1-score** = 0.70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Loading datasets...\n",
      "Training with 100000 examples\n",
      "Loading tokenizer...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, AdamW\n",
    "from transformers import GPT2Config\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import transformers\n",
    "\n",
    "def get_model_size(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    param_size = 4  # Assuming float32 (4 bytes per parameter)\n",
    "    total_size = total_params * param_size\n",
    "    return total_size / (1024 ** 2)  # Convert bytes to MB\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n",
    "\n",
    "class PretrainedGPTEmbedding(nn.Module):\n",
    "    def __init__(self, model_name='gpt2', d_model=512, freeze_embeddings=True):\n",
    "        super(PretrainedGPTEmbedding, self).__init__()\n",
    "        \n",
    "        # Load pretrained GPT model\n",
    "        self.gpt_model = transformers.GPT2Model.from_pretrained(model_name)\n",
    "        \n",
    "        # Freeze embeddings if specified\n",
    "        if freeze_embeddings:\n",
    "            for param in self.gpt_model.wte.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.gpt_model.wpe.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Embedding dimensions and vocab size from the pretrained model\n",
    "        self.d_model = self.gpt_model.config.n_embd\n",
    "        self.vocab_size = self.gpt_model.config.vocab_size\n",
    "        self.linear = nn.Linear(768, d_model)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # Get embeddings directly from pretrained GPT model\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "        \n",
    "        # Use the pretrained model's embedding method\n",
    "        outputs = self.gpt_model.wte(input_ids) + self.gpt_model.wpe(\n",
    "            torch.arange(input_ids.size(1), device=input_ids.device).unsqueeze(0).repeat(input_ids.size(0), 1)\n",
    "        )\n",
    "        \n",
    "        return self.linear(outputs)\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Compute positional encodings\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = self.query(query)\n",
    "        K = self.key(key)\n",
    "        V = self.value(value)\n",
    "        \n",
    "        # Split into multiple heads\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attention = torch.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(attention, V)\n",
    "        \n",
    "        # Concatenate heads and project\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        return self.fc_out(out)\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        \n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, memory, src_mask=None, tgt_mask=None):\n",
    "        # Self-attention\n",
    "        attn_output = self.self_attn(x, x, x, mask=tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Cross-attention with encoder memory\n",
    "        cross_attn_output = self.cross_attn(x, memory, memory, mask=src_mask)\n",
    "        x = self.norm2(x + self.dropout(cross_attn_output))\n",
    "        \n",
    "        # Feed-forward network\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, num_classes, d_model=d_model, num_heads=num_heads, d_ff=d_ff, num_layers=2, dropout=0.1):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        \n",
    "        self.embedding = PretrainedGPTEmbedding('gpt2', d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.fc_out = nn.Linear(d_model, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def generate_mask(self, seq_len):\n",
    "        # Create a causal mask to prevent attending to future tokens\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0)\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, tgt, label=None, infer=False):\n",
    "        # Embedding and positional encoding\n",
    "        memory = torch.zeros(batch_size, seq_len, d_model).to(tgt.device)  # Encoder output\n",
    "\n",
    "        x = self.dropout(self.pos_encoder(self.embedding(tgt)))\n",
    "        \n",
    "        # Create causal mask\n",
    "        tgt_mask = self.generate_mask(tgt.size(1)).to(tgt.device)\n",
    "        \n",
    "        # Pass through decoder layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory)\n",
    "        \n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = self.fc_out(x)\n",
    "        if infer:\n",
    "            return x\n",
    "        # Final projection to vocabulary size\n",
    "        return self.loss(x, label)\n",
    "\n",
    "class YahooDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Combine text fields and clean up\n",
    "        self.texts = (dataframe['question_title'].fillna('') + ' [SEP] ' + \n",
    "                     dataframe['question_content'].fillna('') + ' [SEP] ' + \n",
    "                     dataframe['best_answer'].fillna(''))\n",
    "        \n",
    "        # Convert to zero-based indexing\n",
    "        self.labels = dataframe['class'] - 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts.iloc[idx])\n",
    "        label = self.labels.iloc[idx]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc='Training')\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        label = batch['label'].to(device)\n",
    "        \n",
    "        loss = model(\n",
    "            input_ids,\n",
    "            label=label\n",
    "        )\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Evaluating'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids,\n",
    "                infer=True\n",
    "            )\n",
    "            \n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "    \n",
    "    return classification_report(true_labels, predictions, zero_division=0)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Load datasets\n",
    "print('Loading datasets...')\n",
    "train_df = pd.read_csv('train.csv', \n",
    "                        names=['class', 'question_title', 'question_content', 'best_answer'])\n",
    "\n",
    "# Sample 10,000 examples per class for balanced training\n",
    "samples_per_class = 10000\n",
    "sampled_train_df = []\n",
    "\n",
    "for class_idx in range(1, 11):  # 10 classes\n",
    "    class_data = train_df[train_df['class'] == class_idx]\n",
    "    sampled_class = class_data.sample(n=min(samples_per_class, len(class_data)), \n",
    "                                    random_state=42)\n",
    "    sampled_train_df.append(sampled_class)\n",
    "\n",
    "train_df = pd.concat(sampled_train_df, ignore_index=True)\n",
    "print(f'Training with {len(train_df)} examples')\n",
    "\n",
    "# Load test data\n",
    "test_df = pd.read_csv('test.csv', \n",
    "                        names=['class', 'question_title', 'question_content', 'best_answer'])\n",
    "\n",
    "# Load classes\n",
    "with open('classes.txt', 'r') as f:\n",
    "    class_names = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Initialize tokenizer\n",
    "print('Loading tokenizer...')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = YahooDataset(train_df, tokenizer)\n",
    "test_dataset = YahooDataset(test_df, tokenizer)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32  # Can use larger batch size with smaller model\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    sampler=RandomSampler(train_dataset)\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 137450762\n",
      "Trainable parameters: 98066954\n",
      "Model size: 524.33 MB\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3125/3125 [10:57<00:00,  4.75it/s, loss=1.0235]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 1.7952\n",
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [04:32<00:00,  6.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.46      0.53      6000\n",
      "           1       0.67      0.74      0.70      6000\n",
      "           2       0.68      0.79      0.73      6000\n",
      "           3       0.55      0.36      0.44      6000\n",
      "           4       0.82      0.81      0.82      6000\n",
      "           5       0.78      0.83      0.80      6000\n",
      "           6       0.54      0.46      0.50      6000\n",
      "           7       0.60      0.64      0.62      6000\n",
      "           8       0.67      0.72      0.70      6000\n",
      "           9       0.64      0.80      0.71      6000\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.66      0.66      0.65     60000\n",
      "weighted avg       0.66      0.66      0.65     60000\n",
      "\n",
      "Saved best model with accuracy: 0.6500\n",
      "\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3125/3125 [17:31<00:00,  2.97it/s, loss=0.9488]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 1.0409\n",
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [04:39<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.52      0.54      6000\n",
      "           1       0.72      0.68      0.70      6000\n",
      "           2       0.67      0.83      0.75      6000\n",
      "           3       0.47      0.53      0.50      6000\n",
      "           4       0.83      0.82      0.83      6000\n",
      "           5       0.84      0.83      0.83      6000\n",
      "           6       0.59      0.50      0.54      6000\n",
      "           7       0.74      0.59      0.65      6000\n",
      "           8       0.70      0.73      0.71      6000\n",
      "           9       0.67      0.78      0.72      6000\n",
      "\n",
      "    accuracy                           0.68     60000\n",
      "   macro avg       0.68      0.68      0.68     60000\n",
      "weighted avg       0.68      0.68      0.68     60000\n",
      "\n",
      "Saved best model with accuracy: 0.6800\n",
      "\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3125/3125 [17:35<00:00,  2.96it/s, loss=1.2348]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.9740\n",
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  16%|█▌        | 302/1875 [00:45<04:00,  6.53it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "seq_len = 1024\n",
    "batch_size = 32\n",
    "num_layers=3\n",
    "\n",
    "# Create decoder\n",
    "model = TransformerDecoder(10, d_model = d_model, num_heads = num_heads, d_ff = d_ff, num_layers=num_layers).to(device)\n",
    "\n",
    "total_params, trainable_params = count_parameters(model)\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n",
    "\n",
    "size_in_mb = get_model_size(model)\n",
    "print(f\"Model size: {size_in_mb:.2f} MB\")\n",
    "model.train()\n",
    "\n",
    "# Training settings\n",
    "epochs = 30\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)  # Slightly higher learning rate\n",
    "total_steps = len(train_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "print('Starting training...')\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'\\nEpoch {epoch + 1}/{epochs}')\n",
    "    \n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
    "    print(f'Average training loss: {train_loss:.4f}')\n",
    "    \n",
    "    print('\\nEvaluating...')\n",
    "    report = evaluate(model, test_loader, device)\n",
    "    print('\\nClassification Report:')\n",
    "    print(report)\n",
    "    \n",
    "    # Save model if it improves\n",
    "    accuracy = float(report.split('\\n')[-2].split()[-2])\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.state_dict(), 'best_yahoo_tf_2layer.pt')\n",
    "        print(f'Saved best model with accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model_size(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    param_size = 4  # Assuming float32 (4 bytes per parameter)\n",
    "    total_size = total_params * param_size\n",
    "    return total_size / (1024 ** 2)  # Convert bytes to MB\n",
    "\n",
    "\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 128\n",
    "seq_len = 1024\n",
    "batch_size = 32\n",
    "\n",
    "# Create decoder\n",
    "model = TransformerDecoder(10).to(device)\n",
    "\n",
    "\n",
    "size_in_mb = get_model_size(model)\n",
    "print(f\"Model size: {size_in_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [01:57<00:00, 15.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.58      0.58      6000\n",
      "           1       0.66      0.80      0.73      6000\n",
      "           2       0.75      0.78      0.77      6000\n",
      "           3       0.55      0.49      0.52      6000\n",
      "           4       0.82      0.87      0.84      6000\n",
      "           5       0.88      0.83      0.85      6000\n",
      "           6       0.67      0.45      0.54      6000\n",
      "           7       0.69      0.68      0.68      6000\n",
      "           8       0.63      0.83      0.72      6000\n",
      "           9       0.81      0.69      0.74      6000\n",
      "\n",
      "    accuracy                           0.70     60000\n",
      "   macro avg       0.70      0.70      0.70     60000\n",
      "weighted avg       0.70      0.70      0.70     60000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 512\n",
    "seq_len = 1024\n",
    "batch_size = 32\n",
    "\n",
    "# Create decoder\n",
    "model = TransformerDecoder(10).to(device)\n",
    "\n",
    "model_path = 'best_yahoo_gpt2_2layer.pt'\n",
    "\n",
    "# Load the state dict\n",
    "state_dict = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "report = evaluate(model, test_loader, device)\n",
    "print('\\nClassification Report:')\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aditya-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
