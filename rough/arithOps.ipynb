{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**trishit code BART**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_excel('ArithOps_Train.xlsx')\n",
    "valid_data = pd.read_excel('ArithOps_Validation.xlsx')\n",
    "\n",
    "ids = {}\n",
    "inv_ids = {}\n",
    "for i in range(9):\n",
    "    ids[\"number\"+str(i)] = '<extra_id_'+str(i+1)+'>'\n",
    "    inv_ids['<extra_id_'+str(i+1)+'>'] = \"number\"+str(i)\n",
    "i+=1\n",
    "for sym in ['+', '-', '*', '/', '%']:\n",
    "    ids[sym] = '<extra_id_'+str(i+1)+'>'\n",
    "    inv_ids['<extra_id_'+str(i+1)+'>'] = sym\n",
    "\n",
    "\n",
    "def prepare_input_output(df, ids):\n",
    "    def replaceT5ids(s, joiner=' '):\n",
    "        s_list = s.split(' ')\n",
    "        for i, w in enumerate(s_list):\n",
    "            if w in ids:\n",
    "                s_list[i] = ids[w]\n",
    "        return joiner.join(s_list)\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    for idx, row in df.iterrows():\n",
    "        desc = replaceT5ids(str(row['Description']))\n",
    "        ques = replaceT5ids(str(row['Question']))\n",
    "\n",
    "        eq = replaceT5ids(str(row['Equation']), joiner='')\n",
    "\n",
    "        input_text = desc+' <extra_id_0> '+ques\n",
    "        output_text = eq\n",
    "        inputs.append(input_text)\n",
    "        outputs.append(output_text)\n",
    "    return inputs, outputs\n",
    "\n",
    "train_inputs, train_outputs = prepare_input_output(train_data, ids)\n",
    "valid_inputs, valid_outputs = prepare_input_output(valid_data, ids)\n",
    "valid_expected_outputs = valid_data['Output'].astype(str).tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<extra_id_10><extra_id_1><extra_id_2>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# 3. Create Dataset\n",
    "class ArithmeticDataset(Dataset):\n",
    "    tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "    def __init__(self, inputs, outputs, max_in_length=512, max_out_length=512, test=False):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "        self.test = test\n",
    "        # self.\n",
    "        self.max_in_length = max_in_length\n",
    "        self.max_out_length = max_out_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text = self.inputs[idx]\n",
    "        output_text = self.outputs[idx]\n",
    "        input_encoding = self.tokenizer(\n",
    "            input_text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_in_length,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        output_encoding = self.tokenizer(\n",
    "            output_text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_out_length,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        labels = output_encoding['input_ids']\n",
    "        labels[labels == self.tokenizer.pad_token_id] = - 100  # Ignore padding tokens in the loss\n",
    "        return {\n",
    "            'input_ids': input_encoding['input_ids'].flatten(),\n",
    "            'inp_attention_mask': input_encoding['attention_mask'].flatten(),\n",
    "            'labels': labels.flatten(),\n",
    "            'out_attention_mask': torch.zeros_like(output_encoding['attention_mask']).flatten(),\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def decode(cls, x):\n",
    "        return cls.tokenizer.decode(\n",
    "            x,\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def encode(cls, x):\n",
    "        return cls.tokenizer.encode(\n",
    "            x,\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = ArithmeticDataset(train_inputs, train_outputs, max_in_length=128, max_out_length=16, test=True)\n",
    "valid_dataset = ArithmeticDataset(valid_inputs, valid_outputs, max_in_length=128, max_out_length=16, test=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: The quick brown fox <extra_id_0> jumps over the lazy dog.\n",
      "Encoded Tokens: tensor([[   37,  1704,  4216,     3, 20400, 32099,  4418,     7,   147,     8,\n",
      "         19743,  1782,     5,     1]])\n",
      "Encoded2 Tokens: tensor([[1]])\n",
      "Decoded Sentence: </s>\n"
     ]
    }
   ],
   "source": [
    "a = ArithmeticDataset(train_inputs, train_outputs)\n",
    "\n",
    "# Input sentence\n",
    "sentence = \"The quick brown fox <extra_id_0> jumps over the lazy dog.\"\n",
    "sentence2 = \"\"\n",
    "\n",
    "# Encode the sentence\n",
    "encoded = ArithmeticDataset.encode(sentence)\n",
    "\n",
    "# Decode it back to a sentence\n",
    "\n",
    "encoded2 = a.tokenizer.encode(\n",
    "            sentence2,\n",
    "            truncation=True,\n",
    "            return_tensors='pt',)\n",
    "decoded = ArithmeticDataset.decode(encoded2[0])\n",
    "\n",
    "print(f\"Original Sentence: {sentence}\")\n",
    "print(f\"Encoded Tokens: {encoded}\")\n",
    "print(f\"Encoded2 Tokens: {encoded2}\")\n",
    "print(f\"Decoded Sentence: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox <extra_id_0> jumps over the lazy dog.'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode_model_logit(y):\n",
    "    out = []\n",
    "    for i in range(y.shape[0]):\n",
    "        stp = i\n",
    "        if y[i].item()==1:\n",
    "            break\n",
    "    sen = ArithmeticDataset.decode(y[:stp])\n",
    "    return (sen)\n",
    "        \n",
    "decode_model_logit(a.tokenizer.encode(\n",
    "    sentence,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors='pt',\n",
    ")[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3323191/3142837086.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128])\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 16])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 16])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([7, 16])\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32)\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "model_name = 't5-small'\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "\n",
    "# Training settings\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.02)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "epochs = 700\n",
    "\n",
    "# Function to calculate loss for validation set\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    output_logits = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['inp_attention_mask'].to(device)\n",
    "            print(attention_mask.shape)\n",
    "            labels = batch['labels'].to(device)\n",
    "            print(labels.shape)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=torch.zeros_like(labels).to(device))\n",
    "            loss = outputs.loss\n",
    "            output_logits.append((torch.argmax(outputs.logits, dim = -1).detach().cpu().numpy(), labels.detach().cpu().numpy()))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(data_loader), output_logits\n",
    "\n",
    "\n",
    "avg_loss, output_logits = evaluate_model(model, valid_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label:  + <extra_id_1> <extra_id_2>\n",
      "pred:  \n",
      "\n",
      "label:  + <extra_id_1> <extra_id_2>\n",
      "pred:  + + + + + + + + + + + + + + +\n",
      "\n",
      "label:  - <extra_id_1> <extra_id_2>\n",
      "pred:  \n",
      "\n",
      "label:  - <extra_id_1> <extra_id_2>\n",
      "pred:  \n",
      "\n",
      "label:  - <extra_id_1> <extra_id_2>\n",
      "pred:  \n",
      "\n",
      "label:  - <extra_id_1> <extra_id_2>\n",
      "pred:  \n",
      "\n",
      "label:  - <extra_id_1> <extra_id_2>\n",
      "pred:  \n",
      "\n",
      "label:  - <extra_id_1> <extra_id_2>\n",
      "pred:  \n",
      "\n",
      "label:  - <extra_id_1> <extra_id_2>\n",
      "pred:  \n",
      "\n",
      "label:  - <extra_id_1> <extra_id_2>\n",
      "pred:  \n",
      "\n",
      "label:  - <extra_id_1> <extra_id_2>\n",
      "pred:  \n",
      "\n",
      "label:  - <extra_id_1> <extra_id_2>\n",
      "pred:  \n",
      "\n",
      "label:  - <extra_id_1> <extra_id_2>\n",
      "pred:  \n",
      "\n",
      "label:  - <extra_id_1> <extra_id_2>\n",
      "pred:  \n",
      "\n",
      "label:  + <extra_id_1> <extra_id_2>\n",
      "pred:  + + + + + + + + + + + + + + +\n",
      "\n",
      "label:  + <extra_id_1> <extra_id_2>\n",
      "pred:  + + + + + + + + + + + + + + +\n",
      "\n",
      "label:  - <extra_id_1> <extra_id_2>\n",
      "pred:  \n",
      "\n",
      "label:  + <extra_id_1> <extra_id_2>\n",
      "pred:  + + + + + + + + + + + + + + +\n",
      "\n",
      "label:  + <extra_id_1> <extra_id_2>\n",
      "pred:  \n",
      "\n",
      "label:  - <extra_id_1> <extra_id_2>\n",
      "pred:  \n",
      "\n",
      "label:  - <extra_id_1> <extra_id_2>\n",
      "pred:  \n",
      "\n",
      "label:  - <extra_id_1> <extra_id_2>\n",
      "pred:  \n",
      "\n",
      "label:  - <extra_id_1> <extra_id_2>\n",
      "pred:  \n",
      "\n",
      "label:  - <extra_id_1> <extra_id_2>\n",
      "pred:  \n",
      "\n",
      "label:  - <extra_id_1> <extra_id_2>\n",
      "pred:  \n",
      "\n",
      "label:  - <extra_id_1> <extra_id_2>\n",
      "pred:  \n",
      "\n",
      "label:  + <extra_id_1> <extra_id_2>\n",
      "pred:  \n",
      "\n",
      "label:  - <extra_id_1> <extra_id_2>\n",
      "pred:  \n",
      "\n",
      "label:  - <extra_id_1> <extra_id_2>\n",
      "pred:  \n",
      "\n",
      "label:  - <extra_id_1> <extra_id_2>\n",
      "pred:  \n",
      "\n",
      "label:  - <extra_id_1> <extra_id_2>\n",
      "pred:  \n",
      "\n",
      "label:  - <extra_id_1> <extra_id_2>\n",
      "pred:  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred, lbl = output_logits[0]\n",
    "for i in range(pred.shape[0]):\n",
    "    print('label: ', decode_model_logit(lbl[i]))\n",
    "    print('pred: ', decode_model_logit(pred[i]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "min_loss = 1e100\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_val_loss = evaluate_model(model, valid_loader, device)\n",
    "    \n",
    "    if avg_val_loss<min_loss:\n",
    "        min_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "    print(f\"Train loss: {avg_train_loss}\")\n",
    "    print(f\"Validation loss: {avg_val_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1768, 32098, 32097,     1,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100],\n",
       "        [    3,    18, 32098, 32097,     1,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100],\n",
       "        [ 1636, 32098, 32097, 32096,     1,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100],\n",
       "        [    3,    18, 32098, 32097,     1,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100],\n",
       "        [ 1768, 32097, 32098,     1,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100],\n",
       "        [    3,    87, 32098, 32097,     1,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100],\n",
       "        [ 1768, 32098, 32097,     1,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100],\n",
       "        [ 1429, 32098, 32097,     1,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100],\n",
       "        [ 1768, 32098, 32097,     1,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100],\n",
       "        [    3,    87, 32097, 32098,     1,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100],\n",
       "        [ 1429, 32098, 32097,     1,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100],\n",
       "        [ 1429, 32098, 32097,     1,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100]], device='cuda:0')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred:  + <extra_id_1> <extra_id_2> </s></s>+  + + + + + + + + +\n",
      "lebl:  + <extra_id_1> <extra_id_2> </s>\n",
      "\n",
      "pred:  - <extra_id_1> <extra_id_2> </s></s></s>+ +    *  +\n",
      "lebl:  - <extra_id_1> <extra_id_2>\n",
      "\n",
      "pred:  <extra_id_1> <extra_id_2> <extra_id_3> </s></s>+      +\n",
      "lebl:  -- <extra_id_1> <extra_id_2> <extra_id_3>\n",
      "\n",
      "pred:  - <extra_id_1> <extra_id_2> </s></s>--\n",
      "lebl:  - <extra_id_1> <extra_id_2>\n",
      "\n",
      "pred:  <extra_id_1> <extra_id_1> </s></s>+  *   +\n",
      "lebl:  + <extra_id_2> <extra_id_1> </s>\n",
      "\n",
      "pred:  / <extra_id_1> <extra_id_2> </s></s></s> </s>\n",
      "lebl:  / <extra_id_1> <extra_id_2>\n",
      "\n",
      "pred:  <extra_id_1> <extra_id_2> </s></s>+    +     +\n",
      "lebl:  + <extra_id_1> <extra_id_2> </s>\n",
      "\n",
      "pred:  * <extra_id_1> <extra_id_2> </s></s>* * *  * * * *  * *\n",
      "lebl:  * <extra_id_1> <extra_id_2> </s>\n",
      "\n",
      "pred:  + <extra_id_1> <extra_id_2> </s></s>+ +  +  + + +\n",
      "lebl:  + <extra_id_1> <extra_id_2> </s>\n",
      "\n",
      "pred:  - <extra_id_1> <extra_id_3> </s></s>*  *      * *\n",
      "lebl:  / <extra_id_2> <extra_id_1>\n",
      "\n",
      "pred:  * <extra_id_1> <extra_id_2> </s></s></s>* * * *  * * * * *\n",
      "lebl:  * <extra_id_1> <extra_id_2> </s>\n",
      "\n",
      "pred:  <extra_id_1> <extra_id_2> </s></s>\n",
      "lebl:  * <extra_id_1> <extra_id_2> </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outp = torch.argmax(outputs.logits, dim = -1)\n",
    "\n",
    "for i in range(outp.size(0)):\n",
    "    print(\"pred: \", a.tokenizer.decode(\n",
    "            outp[i],\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "        ))\n",
    "# for i in range(labels.size(0)):\n",
    "\n",
    "    print(\"lebl: \", a.tokenizer.decode(\n",
    "            labels[i][:4],\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "        ))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_prefix(expressions):\n",
    "    # Split the expression into tokens\n",
    "    evals = []\n",
    "    for expression in expressions:\n",
    "        tokens = expression.split()\n",
    "\n",
    "        # Reverse the tokens to process them from right to left\n",
    "        tokens = tokens[::-1]\n",
    "\n",
    "        # Create an empty stack\n",
    "        stack = []\n",
    "\n",
    "        # Iterate through each token\n",
    "        for token in tokens:\n",
    "            if token.isdigit():  # If it's a number, push it onto the stack\n",
    "                stack.append(int(token))\n",
    "            else:\n",
    "                # The token is an operator, pop two operands from the stack\n",
    "                operand1 = stack.pop()\n",
    "                operand2 = stack.pop()\n",
    "\n",
    "                # Perform the operation based on the token\n",
    "                if token == '+':\n",
    "                    result = operand1 + operand2\n",
    "                elif token == '-':\n",
    "                    result = operand1 - operand2\n",
    "                elif token == '*':\n",
    "                    result = operand1 * operand2\n",
    "                elif token == '/':\n",
    "                    result = operand1 / operand2\n",
    "\n",
    "                # Push the result back onto the stack\n",
    "                stack.append(result)\n",
    "\n",
    "            # The final result will be the only element left in the stack\n",
    "        evals.append(stack.pop())\n",
    "    \n",
    "    return evals\n",
    "\n",
    "\n",
    "# Save predictions on validation set\n",
    "def predict_on_validation(model, data_loader, tokenizer, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    pred_evals = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=128)\n",
    "            pred_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "            predictions.extend(pred_texts)\n",
    "            pred_evals.extend(evaluate_prefix(pred_texts))\n",
    "\n",
    "    return predictions, pred_evals\n",
    "\n",
    "# # Get predictions on validation set\n",
    "# val_predictions, val_evals = predict_on_validation(model, valid_loader, tokenizer, device)\n",
    "\n",
    "# # Add predictions to the validation dataframe and save to Excel\n",
    "# valid_data['Predicted Equation'] = val_predictions\n",
    "# valid_data['Output'] = val_evals\n",
    "# valid_data.to_excel('Validation_Predictions.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
