{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import fft\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.signal import cont2discrete\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class LMUFFTCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, memory_size, seq_len, theta):\n",
    "        super(LMUFFTCell, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.memory_size = memory_size\n",
    "        self.seq_len = seq_len\n",
    "        self.theta = theta\n",
    "\n",
    "        self.W_u = nn.Linear(in_features = input_size, out_features = 1)\n",
    "        self.f_u = nn.ReLU()\n",
    "        self.W_h = nn.Linear(in_features = memory_size + input_size, out_features = hidden_size)\n",
    "        self.f_h = nn.ReLU()\n",
    "\n",
    "        A, B = self.stateSpaceMatrices()\n",
    "        self.register_buffer(\"A\", A) # [memory_size, memory_size]\n",
    "        self.register_buffer(\"B\", B) # [memory_size, 1]\n",
    "\n",
    "        H, fft_H = self.impulse()\n",
    "        self.register_buffer(\"H\", H) # [memory_size, seq_len]\n",
    "        self.register_buffer(\"fft_H\", fft_H) # [memory_size, seq_len + 1]\n",
    "\n",
    "    def stateSpaceMatrices(self):\n",
    "        \"\"\" Returns the discretized state space matrices A and B \"\"\"\n",
    "\n",
    "        Q = np.arange(self.memory_size, dtype = np.float64).reshape(-1, 1)\n",
    "        R = (2*Q + 1) / self.theta\n",
    "        i, j = np.meshgrid(Q, Q, indexing = \"ij\")\n",
    "\n",
    "        # Continuous\n",
    "        A = R * np.where(i < j, -1, (-1.0)**(i - j + 1))\n",
    "        B = R * ((-1.0)**Q)\n",
    "        C = np.ones((1, self.memory_size))\n",
    "        D = np.zeros((1,))\n",
    "\n",
    "        # Convert to discrete\n",
    "        A, B, C, D, dt = cont2discrete(\n",
    "            system = (A, B, C, D), \n",
    "            dt = 1.0, \n",
    "            method = \"zoh\"\n",
    "        )\n",
    "\n",
    "        # To torch.tensor\n",
    "        A = torch.from_numpy(A).float() # [memory_size, memory_size]\n",
    "        B = torch.from_numpy(B).float() # [memory_size, 1]\n",
    "        \n",
    "        return A, B\n",
    "\n",
    "    def impulse(self):\n",
    "        \"\"\" Returns the matrices H and the 1D Fourier transform of H (Equations 23, 26 of the paper) \"\"\"\n",
    "\n",
    "        H = []\n",
    "        A_i = torch.eye(self.memory_size).to(self.A.device) \n",
    "        for t in range(self.seq_len):\n",
    "            H.append(A_i @ self.B)\n",
    "            A_i = self.A @ A_i\n",
    "\n",
    "        H = torch.cat(H, dim = -1) # [memory_size, seq_len]\n",
    "        fft_H = fft.rfft(H, n = 2*self.seq_len, dim = -1) # [memory_size, seq_len + 1]\n",
    "\n",
    "        return H, fft_H\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            x (torch.tensor): \n",
    "                Input of size [batch_size, seq_len, input_size]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, input_size = x.shape\n",
    "        # print(\"batch_size, seq_len, input_size\", batch_size, seq_len, input_size)\n",
    "\n",
    "        # Equation 18 of the paper\n",
    "        u = self.f_u(self.W_u(x)) # [batch_size, seq_len, 1]\n",
    "\n",
    "        # Equation 26 of the paper\n",
    "        fft_input = u.permute(0, 2, 1) # [batch_size, 1, seq_len]\n",
    "        fft_u = fft.rfft(fft_input, n = 2*seq_len, dim = -1) # [batch_size, seq_len, seq_len+1]\n",
    "\n",
    "        # Element-wise multiplication (uses broadcasting)\n",
    "        # [batch_size, 1, seq_len+1] * [1, memory_size, seq_len+1]\n",
    "        temp = fft_u * self.fft_H.unsqueeze(0) # [batch_size, memory_size, seq_len+1]\n",
    "\n",
    "        m = fft.irfft(temp, n = 2*seq_len, dim = -1) # [batch_size, memory_size, seq_len+1]\n",
    "        m = m[:, :, :seq_len] # [batch_size, memory_size, seq_len]\n",
    "        m = m.permute(0, 2, 1) # [batch_size, seq_len, memory_size]\n",
    "\n",
    "        # Equation 20 of the paper (W_m@m + W_x@x  W@[m;x])\n",
    "        input_h = torch.cat((m, x), dim = -1) # [batch_size, seq_len, memory_size + input_size]\n",
    "        h = self.f_h(self.W_h(input_h)) # [batch_size, seq_len, hidden_size]\n",
    "\n",
    "        h_n = h[:, -1, :] # [batch_size*T, hidden_size]\n",
    "\n",
    "        return h, h_n\n",
    "    \n",
    "    def forward_recurrent(self, x, m_last):\n",
    "        u = self.f_u(self.W_u(x)) # [batch_size, seq_len, 1]\n",
    "        # A: torch.Size([512, 512]), m_last: torch.Size([256, 512]), B: torch.Size([512, 1]), u: torch.Size([256, 1])\n",
    "        m = m_last @ self.A.T + u @ self.B.T  # [batch_size, memory_size]\n",
    "        input_h = torch.cat((m, x), dim = -1) # [batch_size, seq_len, memory_size + input_size]\n",
    "        h = self.f_h(self.W_h(input_h)) # [batch_size, seq_len, hidden_size]\n",
    "\n",
    "        return h, m\n",
    "\n",
    "class LMU(nn.Module):\n",
    "    def __init__(self, dim, T, use_all_h=True):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.hidden_size = dim\n",
    "        self.memory_size = dim\n",
    "        self.use_all_h = use_all_h\n",
    "        self.lmu = LMUFFTCell(input_size=dim, hidden_size=self.hidden_size, memory_size=self.memory_size, seq_len=T, theta=T)\n",
    "        # self.lmu = LMUFFTCell(input_size=dim, hidden_size=self.hidden_size, memory_size=self.memory_size, seq_len=64, theta=64)\n",
    "\n",
    "        self.proj_conv = nn.Conv1d(dim, dim, kernel_size=1, stride=1)\n",
    "        self.proj_bn = nn.BatchNorm1d(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(-1,-2).contiguous() # B, C, N -> B, N, C\n",
    "        h, _ = self.lmu(x) # B, N, C; B, C\n",
    "        \n",
    "        x = h.transpose(-1,-2).contiguous() #if self.use_all_h else h_n.unsqueeze(-1) # h or h_n\n",
    "\n",
    "        x = self.proj_conv(x)\n",
    "        x = self.proj_bn(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class LinearFFN(nn.Module):\n",
    "    def __init__(self, in_features, pre_norm=False, hidden_features=None, out_features=None, drop=0., act_type='spike'):\n",
    "        super().__init__()\n",
    "        self.pre_norm = pre_norm\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "\n",
    "        self.fc1_linear  = nn.Linear(in_features, hidden_features)\n",
    "        self.fc1_ln = nn.LayerNorm(hidden_features)\n",
    "        self.fc1_lif = get_act(act_type if act_type == 'spike' else 'gelu', tau=2.0, detach_reset=True)\n",
    "\n",
    "        self.fc2_linear = nn.Linear(hidden_features, out_features)\n",
    "        self.fc2_ln = nn.LayerNorm(out_features)\n",
    "        self.fc2_lif = get_act(act_type, tau=2.0, detach_reset=True)\n",
    " \n",
    "        self.c_hidden = hidden_features\n",
    "        self.c_output = out_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,C,N = x.shape\n",
    "        # \n",
    "        x = x.permute(0,2,1) # B, N, C\n",
    "        # x = x.reshape(B*N, C)\n",
    "        if self.pre_norm:\n",
    "            x = self.fc1_ln(x)\n",
    "            x = self.fc1_lif(x)\n",
    "            x = self.fc1_linear(x)\n",
    "            \n",
    "            x = self.fc2_ln(x)\n",
    "            x = self.fc2_lif(x)\n",
    "            x = self.fc2_linear(x)\n",
    "\n",
    "        else:\n",
    "            x = self.fc1_linear(x)\n",
    "            x = self.fc1_ln(x)\n",
    "            x = self.fc1_lif(x)\n",
    "\n",
    "            x = self.fc2_linear(x)\n",
    "            x = self.fc2_ln(x)\n",
    "            x = self.fc2_lif(x)\n",
    "\n",
    "        # x = x.reshape(B, N, self.c_output)\n",
    "        x = x.permute(0,2,1) # B, C, N\n",
    "        return x\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, T, mlp_ratio=4., act_type='spike'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = LMU(dim=dim, T=T)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = LinearFFN(in_features=dim, hidden_features=mlp_hidden_dim, act_type=act_type)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(x)\n",
    "        x = x + self.mlp(x)\n",
    "        return x\n",
    "\n",
    "class perm(nn.Module):\n",
    "    def __init__(self, a, b, c) -> None:\n",
    "        super().__init__()\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.permute(self.a,self.b,self.c).contiguous()\n",
    "\n",
    "def get_act(act_type = 'spike', **act_params):\n",
    "    '''\n",
    "    act_type :- spike, gelu, relu, identity\n",
    "\n",
    "    output :- class <act_type>\n",
    "    '''\n",
    "    act_type = act_type.lower()\n",
    "    # if act_type == 'spike':\n",
    "    #     return MultiStepLIFNode(**act_params, backend='cupy')\n",
    "    #     # act_params['init_tau'] = act_params.pop('tau')\n",
    "    #     # return MultiStepParametricLIFNode(**act_params, backend=\"cupy\")\n",
    "    if act_type == 'relu':\n",
    "        return nn.ReLU()\n",
    "    elif act_type == 'gelu':\n",
    "        return nn.GELU()\n",
    "    elif act_type == 'identity':\n",
    "        return nn.Identity()\n",
    "    \n",
    "def get_conv_block(T, dim, act_type, kernel_size=3, padding=1, groups=1):\n",
    "    return [\n",
    "        perm(0,2,1),\n",
    "        nn.Conv1d(dim, dim, kernel_size=kernel_size, stride=1, padding=padding, groups=groups, bias=False),\n",
    "        nn.BatchNorm1d(dim),\n",
    "        perm(1,2,0),\n",
    "        get_act(act_type, tau=2.0, detach_reset=True),\n",
    "        perm(2,1,0)\n",
    "]\n",
    "\n",
    "class Conv1d4EB(nn.Module):\n",
    "    def __init__(self, T=128, vw_dim=256, act_type='spike'):\n",
    "        super().__init__()\n",
    "\n",
    "        kernel_size = 3\n",
    "        padding = 1\n",
    "        groups = 1\n",
    "        self.proj_conv = nn.ModuleList(\n",
    "            [perm(0,2,1)]+\\\n",
    "            get_conv_block(T, vw_dim, act_type)+\\\n",
    "            get_conv_block(T, vw_dim, act_type, kernel_size=kernel_size, padding=padding, groups=groups)+\\\n",
    "            get_conv_block(T, vw_dim, act_type, kernel_size=kernel_size, padding=padding, groups=groups)+\\\n",
    "            get_conv_block(T, vw_dim, act_type, kernel_size=kernel_size, padding=padding, groups=groups)+\\\n",
    "            [perm(0,2,1)]\n",
    "        )\n",
    "        self.rpe_conv = nn.ModuleList(\n",
    "            [perm(0,2,1)]+\\\n",
    "            get_conv_block(T, vw_dim, act_type, kernel_size=kernel_size, padding=padding, groups=groups)+\\\n",
    "            [perm(0,2,1)]\n",
    "        )\n",
    "        self.act_loss = 0.0\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        for ele in self.proj_conv:\n",
    "            x = ele(x)\n",
    "\n",
    "        x_rpe = x.clone()\n",
    "        for ele in self.rpe_conv:\n",
    "            x_rpe = ele(x_rpe)\n",
    "\n",
    "        x = x + x_rpe\n",
    "        \n",
    "        return x \n",
    "\n",
    "class LMU_RNN(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, hidden_size, act_type='relu', T=784, test_mode='all_seq',with_head_lif=False):\n",
    "        super().__init__()\n",
    "        self.with_head_lif = with_head_lif\n",
    "        self.test_mode = test_mode\n",
    "\n",
    "        self.in_layer = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        self.patch_embed = Conv1d4EB(T=T, vw_dim=hidden_size, act_type=act_type)\n",
    "\n",
    "        self.block = nn.ModuleList([\n",
    "            Block(dim=hidden_size, T=T, act_type=act_type)\n",
    "            for j in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # classification head\n",
    "        if self.with_head_lif:\n",
    "            self.head_bn = nn.BatchNorm1d(hidden_size)\n",
    "            self.head_lif = get_act(act_type, tau=2.0, detach_reset=True)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        for blk in self.block:\n",
    "            x = blk(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.act_loss = 0.0\n",
    "        x = self.in_layer(x)\n",
    "        x = x.permute(0, 2, 1).contiguous()\n",
    "        x = self.forward_features(x)    # b, d, t -> b, d, t\n",
    "\n",
    "        if self.with_head_lif:\n",
    "            x = self.head_bn(x)         # b, d, t \n",
    "            x = self.head_lif(x)        # b, d, t\n",
    "\n",
    "        x = x.permute(0, 2, 1).contiguous()\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistLMU(nn.Module):\n",
    "    r\"\"\"\n",
    "    Very Simple 2 layer LSTM with an fc layer on last steps hidden dimension\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, codebook_size):\n",
    "        super(MnistLMU, self).__init__()\n",
    "        self.rnn = LMU_RNN(input_size=2, hidden_size=128, num_layers=2, T=32)\n",
    "\n",
    "        self.fc = nn.Sequential(nn.Linear(hidden_size, hidden_size//4),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(hidden_size//4, codebook_size))\n",
    "        # Add pad and start token to embedding size\n",
    "        self.word_embedding = nn.Embedding(codebook_size+2, input_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.word_embedding(x)\n",
    "        output = self.rnn(x)\n",
    "        output = output[:, -1, :]\n",
    "        return self.fc(output)\n",
    "\n",
    "class MnistLSTM(nn.Module):\n",
    "    r\"\"\"\n",
    "    Very Simple 2 layer LSTM with an fc layer on last steps hidden dimension\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, codebook_size):\n",
    "        super(MnistLSTM, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size=2, hidden_size=128, num_layers=2, batch_first=True)\n",
    "        self.fc = nn.Sequential(nn.Linear(hidden_size, hidden_size // 4),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(hidden_size // 4, codebook_size))\n",
    "        # Add pad and start token to embedding size\n",
    "        self.word_embedding = nn.Embedding(codebook_size+2, input_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.word_embedding(x)\n",
    "        output, _ = self.rnn(x)\n",
    "        output = output[:, -1, :]\n",
    "        return self.fc(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 136.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60000 images for split train\n",
      "getting statistics of data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:29<00:00, 14.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from dataset.mnist_dataset import MnistDataset\n",
    "from utils.inception_score import inceptionScore\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import fft\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.signal import cont2discrete\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "mnist = MnistDataset('train', 'data/train/images', im_channels=3)\n",
    "\n",
    "FID = inceptionScore(mnist, device, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from model.vqvae import get_model\n",
    "from tools.train_lstm import MnistLSTM, MnistLMU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "\n",
    "def generate(config):\n",
    "    r\"\"\"\n",
    "    Method for generating images after training vqvae and lstm\n",
    "    1. Create config\n",
    "    2. Create and load vqvae model\n",
    "    3. Create and load LSTM model\n",
    "    4. Generate 100 encoder outputs from trained LSTM\n",
    "    5. Pass them to the trained vqvae decoder\n",
    "    6. Save the generated image\n",
    "    :param args:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    ########## Load VQVAE Model ##############\n",
    "    vqvae_model = get_model(config).to(device)\n",
    "    vqvae_model.to(device)\n",
    "    assert os.path.exists(os.path.join(config['train_params']['task_name'],\n",
    "                                                  config['train_params']['ckpt_name'])), \"Train the vqvae model first\"\n",
    "    vqvae_model.load_state_dict(torch.load(os.path.join(config['train_params']['task_name'],\n",
    "                                                  config['train_params']['ckpt_name']), map_location=device))\n",
    "        \n",
    "    vqvae_model.eval()\n",
    "    #########################################\n",
    "    \n",
    "    ################ Generate Samples #############\n",
    "    generated_quantized_indices = []\n",
    "    mnist_encodings = pickle.load(open(os.path.join(config['train_params']['task_name'],\n",
    "                                                    config['train_params']['output_train_dir'],\n",
    "                                                    'mnist_encodings.pkl'), 'rb'))\n",
    "    mnist_encodings_length = mnist_encodings.reshape(mnist_encodings.size(0), -1).shape[-1]\n",
    "    #########################################\n",
    "\n",
    "    ########## Load LSTM ##############\n",
    "    default_lstm_config = {\n",
    "        'input_size': 2,\n",
    "        'hidden_size': 128,\n",
    "        'codebook_size': config['model_params']['codebook_size']\n",
    "    }\n",
    "    \n",
    "    if config['model_params']['rnn_type']=='lstm':\n",
    "        model = MnistLSTM(input_size=default_lstm_config['input_size'],\n",
    "                        hidden_size=default_lstm_config['hidden_size'],\n",
    "                        codebook_size=default_lstm_config['codebook_size']).to(device)\n",
    "    elif config['model_params']['rnn_type']=='lmu':\n",
    "        model = MnistLMU(input_size=default_lstm_config['input_size'],\n",
    "                        hidden_size=default_lstm_config['hidden_size'],\n",
    "                        codebook_size=default_lstm_config['codebook_size'],\n",
    "                        T=64).to(device)\n",
    "    model.to(device)\n",
    "    assert os.path.exists(os.path.join(config['train_params']['task_name'],\n",
    "                                                    'best_mnist_lstm.pth')), \"Train the lstm first\"\n",
    "    model.load_state_dict(torch.load(os.path.join(config['train_params']['task_name'],\n",
    "                                                    f'best_mnist_{config['model_params']['rnn_type']}.pth'), map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    # Assume fixed contex size\n",
    "    context_size = 64\n",
    "    num_samples = 1000\n",
    "    print('Generating Samples', mnist_encodings_length)\n",
    "    for _ in tqdm(range(num_samples)):\n",
    "        # Initialize with start token\n",
    "        ctx = torch.ones((1)).to(device) * (config['model_params']['codebook_size'])\n",
    "        \n",
    "        for i in range(mnist_encodings_length):\n",
    "            padded_ctx = ctx\n",
    "            if len(ctx) < context_size:\n",
    "                # Pad context with pad token\n",
    "                padded_ctx = torch.nn.functional.pad(padded_ctx, (0, context_size - len(ctx)), \"constant\",\n",
    "                                                  config['model_params']['codebook_size']+1)\n",
    "            # print(padded_ctx[None, :].shape)\n",
    "            out = model(padded_ctx[None, :][:,-64:].long().to(device))\n",
    "            probs = torch.nn.functional.softmax(out, dim=-1)\n",
    "            pred = torch.multinomial(probs[0], num_samples=1)\n",
    "            # Update the context with the new prediction\n",
    "            ctx = torch.cat([ctx, pred])\n",
    "            # print(padded_ctx.shape, pred.shape, ctx.shape)\n",
    "        generated_quantized_indices.append(ctx[1:][None, :])\n",
    "    \n",
    "    ######## Decode the Generated Indices ##########\n",
    "    generated_quantized_indices = torch.cat(generated_quantized_indices, dim=0)\n",
    "    h = int(generated_quantized_indices[0].size(-1)**0.5)\n",
    "    quantized_indices = generated_quantized_indices.reshape((generated_quantized_indices.size(0), h, h)).long()\n",
    "    quantized_indices = torch.nn.functional.one_hot(quantized_indices, config['model_params']['codebook_size'])\n",
    "    quantized_indices = quantized_indices.permute((0, 3, 1, 2))\n",
    "    output = vqvae_model.decode_from_codebook_indices(quantized_indices.float())\n",
    "    \n",
    "    # Transform from -1, 1 range to 0,1\n",
    "    output = (output + 1) / 2\n",
    "    \n",
    "    if config['model_params']['in_channels'] == 3:\n",
    "        # Just because we took input as cv2.imread which is BGR so make it RGB\n",
    "        output = output[:, [2, 1, 0], :, :]\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LMU model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_params': {'in_channels': 3, 'convbn_blocks': 4, 'conv_kernel_size': [3, 3, 3, 2], 'conv_kernel_strides': [2, 2, 1, 1], 'convbn_channels': [3, 16, 32, 8, 8], 'conv_activation_fn': 'leaky', 'transpose_bn_blocks': 4, 'transposebn_channels': [8, 8, 32, 16, 3], 'transpose_kernel_size': [3, 4, 4, 4], 'transpose_kernel_strides': [1, 2, 1, 1], 'transpose_activation_fn': 'leaky', 'latent_dim': 8, 'codebook_size': 20, 'rnn_type': 'lmu'}, 'train_params': {'task_name': 'vqvae_latent_8_colored_codebook_20', 'batch_size': 64, 'epochs': 20, 'lr': 0.005, 'crit': 'l2', 'reconstruction_loss_weight': 5, 'codebook_loss_weight': 1, 'commitment_loss_weight': 0.2, 'ckpt_name': 'best_vqvae_latent_8_colored_codebook_20.pth', 'seed': 111, 'save_training_image': True, 'train_path': 'data/train/images', 'test_path': 'data/test/images', 'output_train_dir': 'output'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1190324/4123725763.py:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vqvae_model.load_state_dict(torch.load(os.path.join(config['train_params']['task_name'],\n",
      "/tmp/ipykernel_1190324/4123725763.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(config['train_params']['task_name'],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Samples 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:39<00:00,  6.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 20, 8, 8])\n",
      "torch.Size([1000, 8, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "config = {'model_params': {'in_channels': 3, \n",
    "                           'convbn_blocks': 4, \n",
    "                           'conv_kernel_size': [3, 3, 3, 2], \n",
    "                           'conv_kernel_strides': [2, 2, 1, 1], \n",
    "                           'convbn_channels': [3, 16, 32, 8, 8], \n",
    "                           'conv_activation_fn': 'leaky', \n",
    "                           'transpose_bn_blocks': 4, \n",
    "                           'transposebn_channels': [8, 8, 32, 16, 3], \n",
    "                           'transpose_kernel_size': [3, 4, 4, 4], \n",
    "                           'transpose_kernel_strides': [1, 2, 1, 1], \n",
    "                           'transpose_activation_fn': 'leaky', \n",
    "                           'latent_dim': 8, 'codebook_size': 20, \n",
    "                           'rnn_type': 'lmu'}, \n",
    "          'train_params': {'task_name': 'vqvae_latent_8_colored_codebook_20', \n",
    "                           'batch_size': 64, 'epochs': 20, 'lr': 0.005, \n",
    "                           'crit': 'l2', 'reconstruction_loss_weight': 5, \n",
    "                           'codebook_loss_weight': 1, \n",
    "                           'commitment_loss_weight': 0.2, \n",
    "                           'ckpt_name': 'best_vqvae_latent_8_colored_codebook_20.pth', \n",
    "                           'seed': 111, 'save_training_image': True, \n",
    "                           'train_path': 'data/train/images', \n",
    "                           'test_path': 'data/test/images', 'output_train_dir': 'output'}}\n",
    "\n",
    "out = generate(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating statistics of generated data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:29<00:00, 14.79s/it]\n"
     ]
    }
   ],
   "source": [
    "fid = FID.calculate_fid_for_generatorSamples(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.29911003394864"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fid.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1190324/4123725763.py:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vqvae_model.load_state_dict(torch.load(os.path.join(config['train_params']['task_name'],\n",
      "/tmp/ipykernel_1190324/4123725763.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(config['train_params']['task_name'],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_params': {'in_channels': 3, 'convbn_blocks': 4, 'conv_kernel_size': [3, 3, 3, 2], 'conv_kernel_strides': [2, 2, 1, 1], 'convbn_channels': [3, 16, 32, 8, 8], 'conv_activation_fn': 'leaky', 'transpose_bn_blocks': 4, 'transposebn_channels': [8, 8, 32, 16, 3], 'transpose_kernel_size': [3, 4, 4, 4], 'transpose_kernel_strides': [1, 2, 1, 1], 'transpose_activation_fn': 'leaky', 'latent_dim': 8, 'codebook_size': 20, 'rnn_type': 'lstm'}, 'train_params': {'task_name': 'vqvae_latent_8_colored_codebook_20', 'batch_size': 64, 'epochs': 20, 'lr': 0.005, 'crit': 'l2', 'reconstruction_loss_weight': 5, 'codebook_loss_weight': 1, 'commitment_loss_weight': 0.2, 'ckpt_name': 'best_vqvae_latent_8_colored_codebook_20.pth', 'seed': 111, 'save_training_image': True, 'train_path': 'data/train/images', 'test_path': 'data/test/images', 'output_train_dir': 'output'}}\n",
      "Generating Samples 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:31<00:00, 31.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 20, 8, 8])\n",
      "torch.Size([1000, 8, 8, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "config = {'model_params': {'in_channels': 3, \n",
    "                           'convbn_blocks': 4, \n",
    "                           'conv_kernel_size': [3, 3, 3, 2], \n",
    "                           'conv_kernel_strides': [2, 2, 1, 1], \n",
    "                           'convbn_channels': [3, 16, 32, 8, 8], \n",
    "                           'conv_activation_fn': 'leaky', \n",
    "                           'transpose_bn_blocks': 4, \n",
    "                           'transposebn_channels': [8, 8, 32, 16, 3], \n",
    "                           'transpose_kernel_size': [3, 4, 4, 4], \n",
    "                           'transpose_kernel_strides': [1, 2, 1, 1], \n",
    "                           'transpose_activation_fn': 'leaky', \n",
    "                           'latent_dim': 8, 'codebook_size': 20, \n",
    "                           'rnn_type': 'lstm'}, \n",
    "          'train_params': {'task_name': 'vqvae_latent_8_colored_codebook_20', \n",
    "                           'batch_size': 64, 'epochs': 20, 'lr': 0.005, \n",
    "                           'crit': 'l2', 'reconstruction_loss_weight': 5, \n",
    "                           'codebook_loss_weight': 1, \n",
    "                           'commitment_loss_weight': 0.2, \n",
    "                           'ckpt_name': 'best_vqvae_latent_8_colored_codebook_20.pth', \n",
    "                           'seed': 111, 'save_training_image': True, \n",
    "                           'train_path': 'data/train/images', \n",
    "                           'test_path': 'data/test/images', 'output_train_dir': 'output'}}\n",
    "\n",
    "out = generate(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating statistics of generated data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:30<00:00, 15.40s/it]\n"
     ]
    }
   ],
   "source": [
    "fid = FID.calculate_fid_for_generatorSamples(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155.01308510968738"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fid.item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aditya-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
